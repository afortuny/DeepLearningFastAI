{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afortuny/DeepLearningFastAI/blob/main/11_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kzn8OTbthOpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a0b855-fa5d-4d22-c5ac-ba5fc68dbb42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 719 kB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 362 kB 54.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 59.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 43.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 68.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 50.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 47.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 52.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 68.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 50.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 53.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 36.0 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cq9odkmjhOp1"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from fastbook import *\n",
        "from IPython.display import display,HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuuSV4O7hOqJ"
      },
      "source": [
        "# Transfer learning on text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to be able to use unstructured text for a downstream task, we can leverage a pretrained model on massive amounts of text instead of starting from scratch. The models use to encode multiple aspects of language and text in general are called language models."
      ],
      "metadata": {
        "id": "JEppBUK87BFy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1B8e_yRhOqP"
      },
      "source": [
        "\n",
        "\n",
        "What we call a language model is a model that has been trained to guess what the next word in a text is (having read the ones before). This kind of task is called *self-supervised learning*: we do not need to give labels to our model, just feed it lots and lots of texts. It has a process to automatically get labels from the data, and this task isn't trivial: to properly guess the next word in a sentence, the model will have to develop an understanding of the English (or other) language. Self-supervised learning can also be used in other domains; for instance, see [\"Self-Supervised Learning and Computer Vision\"](https://www.fast.ai/2020/01/13/self_supervised/) for an introduction to vision applications. Self-supervised learning is not usually used for the model that is trained directly, but instead is used for pretraining a model used for transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITlY9-hthOqU"
      },
      "source": [
        "> jargon: Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels. For instance, training a model to predict the next word in a text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1Qm2W8OhOqW"
      },
      "source": [
        "The pretrained language model we will used here was pretrained on Wikipedia. We can get great results by directly fine-tuning this language model to our downstream task, but with one extra step, we can do even better. The Wikipedia English is slightly different from language used in your specific application, so instead of jumping directly to the classifier, we could fine-tune our pretrained language model our own corpus and then use *that* as the base for our downstream task.\n",
        "\n",
        "Even if our language model knows the basics of the language we are using in the task (e.g., our pretrained model is in English), it helps to get used to the style of the corpus we are targeting. It may be more informal language, or more technical, with new words to learn or different ways of composing sentences. \n",
        "\n",
        "This is known as the Universal Language Model Fine-tuning (ULMFit) approach. The [paper](https://arxiv.org/abs/1801.06146) showed that this extra stage of fine-tuning of the language model, prior to transfer learning to a classification task, resulted in significantly better predictions. Using this approach, we have three stages for transfer learning in NLP, as summarized here:."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2WJK8vhOqc"
      },
      "source": [
        "<img alt=\"Diagram of the ULMFiT process\" width=\"700\" caption=\"The ULMFiT process\" id=\"ulmfit_process\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00027.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj7c1b9ohOqj"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKA4lU3XhOql"
      },
      "source": [
        "It's not at all obvious how we're going to use what we've learned so far to build a language model. Sentences can be different lengths, and documents can be very long. So, how can we predict the next word of a sentence using a neural network? Let's find out!\n",
        "\n",
        "We've already seen how categorical variables can be used as independent variables for a neural network ([categorical embeddings]([https://alanfortunysicart.blogspot.com/2022/07/fastai-deep-learning-journey-part-10.html)). \n",
        "\n",
        "We can do nearly the same thing with text! What is new is the idea of a sequence. First we concatenate all of the documents in our dataset into one big long string and split it into words, giving us a very long list of words (or \"tokens\"). Our independent variable will be the sequence of words starting with the first word in our very long list and ending with the second to last, and our dependent variable will be the sequence of words starting with the second word and ending with the last word. \n",
        "\n",
        "Our vocab will consist of a mix of common words that are already in the vocabulary of our pretrained model and new words specific to our corpus. Our embedding matrix will be built accordingly: for words that are in the vocabulary of our pretrained model, we will take the corresponding row in the embedding matrix of the pretrained model; but for new words we won't have anything, so we will just initialize the corresponding row with a random vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L91emk9thOqz"
      },
      "source": [
        "### Word Tokenization with fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3K_X33khOq1"
      },
      "source": [
        "Rather than providing its own tokenizers, fastai instead provides a consistent interface to a range of tokenizers in external libraries. Tokenization is an active field of research, and new and improved tokenizers are coming out all the time, so the defaults that fastai uses change too. However, the API and options shouldn't change too much, since fastai tries to maintain a consistent API even as the underlying technology changes.\n",
        "\n",
        "Let's try it out with the good reads data set [good books dataset](https://www.kaggle.com/datasets/meetnaren/goodreads-best-books) containing book descriptions and ratings:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib \n",
        "path = pathlib.Path('/content/gdrive/MyDrive/NLP/')"
      ],
      "metadata": {
        "id": "fPmhgt8YRIVm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip '/content/gdrive/MyDrive/NLP/book_data.csv (2).zip' -d '/content/gdrive/MyDrive/NLP'"
      ],
      "metadata": {
        "id": "7JJwn_tetSWY",
        "outputId": "4a661db9-7dcc-46f2-864a-8e8fa3511879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/NLP/book_data.csv (2).zip\n",
            "  inflating: /content/gdrive/MyDrive/NLP/book_data.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the labels\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/NLP/book_data.csv')"
      ],
      "metadata": {
        "id": "od7UvHrutcaB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keep only the description and rating\n",
        "df = df[['book_desc','book_rating']]"
      ],
      "metadata": {
        "id": "enicyixyX_bq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop na recors\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "00byCw6HYkad"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#validate same amount of complete records\n",
        "df.count()"
      ],
      "metadata": {
        "id": "-YF-LnAiYT51",
        "outputId": "14664e3d-324c-4114-d4cd-e2268951ea70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "book_desc      52970\n",
              "book_rating    52970\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this data set contains multiple languages, the following code filter out the ones in english"
      ],
      "metadata": {
        "id": "ngMjbR5U-lds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# install and import languague detector\n",
        "!pip install langdetect\n",
        "from langdetect import detect"
      ],
      "metadata": {
        "id": "X8WFTBqPtspp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detect english descriptions and keep only those\n",
        "def detect_en(text):\n",
        "    try:\n",
        "        return detect(text) == 'en'\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "df = df[df['book_desc'].apply(detect_en)]"
      ],
      "metadata": {
        "id": "o6dKII4DxkUv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the data frame after processing, as language detect take a few min"
      ],
      "metadata": {
        "id": "I0HX5HOW-tti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df.to_pickle(\"/content/gdrive/MyDrive/NLP/english_books.pkl\")"
      ],
      "metadata": {
        "id": "7Qz4Xn6Nq59T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_pickle(\"/content/gdrive/MyDrive/NLP/english_books.pkl\")"
      ],
      "metadata": {
        "id": "S7uJTyzKt1D5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWwbJEQghOrL"
      },
      "source": [
        "As we write this post, the default English word tokenizer for fastai uses a library called *spaCy*. It has a sophisticated rules engine with special rules for URLs, individual special English words, and much more. Rather than directly using `SpacyTokenizer`, however, we'll use `WordTokenizer`, since that will always point to fastai's current default word tokenizer (which may not necessarily be spaCy, depending when you're reading this)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = df.loc[0,'book_desc'][:50]"
      ],
      "metadata": {
        "id": "NqF0kUwZ1N-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mZxWW4whOrN",
        "outputId": "7df81a74-89ab-4c92-9bb5-c23a50915619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(#9) ['Winning','will','make','you','famous','.','Losing','means','certain']\n"
          ]
        }
      ],
      "source": [
        "spacy = WordTokenizer()\n",
        "toks = first(spacy([txt]))\n",
        "print(coll_repr(toks, 30))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1txpH5O0hOrO"
      },
      "source": [
        "As you see, spaCy has mainly just separated out the words and punctuation. But it does something else here too: it has split \"it's\" into \"it\" and \"'s\". That makes intuitive sense; these are separate words, really. Tokenization is a surprisingly subtle task, when you think about all the little details that have to be handled. Fortunately, spaCy handles these pretty well for us—for instance, here we see that \".\" is separated when it terminates a sentence, but not in an acronym or number:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRoQQBA4hOrZ"
      },
      "source": [
        "fastai then adds some additional functionality to the tokenization process with the `Tokenizer` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP7b_sNIhOra",
        "outputId": "32dbb2d8-d498-4a5e-ba6e-2d8ba056df67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(#12) ['xxbos','xxmaj','winning','will','make','you','famous','.','xxmaj','losing','means','certain']\n"
          ]
        }
      ],
      "source": [
        "tkn = Tokenizer(spacy)\n",
        "print(coll_repr(tkn(txt), 31))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNCgYXYahOrc"
      },
      "source": [
        "Notice that there are now some tokens that start with the characters \"xx\", which is not a common word prefix in English. These are *special tokens*.\n",
        "\n",
        "For example, the first item in the list, `xxbos`, is a special token that indicates the start of a new text (\"BOS\" is a standard NLP acronym that means \"beginning of stream\"). By recognizing this start token, the model will be able to learn it needs to \"forget\" what was said previously and focus on upcoming words.\n",
        "\n",
        "These special tokens don't come from spaCy directly. They are there because fastai adds them by default, by applying a number of rules when processing text. These rules are designed to make it easier for a model to recognize the important parts of a sentence. In a sense, we are translating the original English language sequence into a simplified tokenized language—a language that is designed to be easy for a model to learn.\n",
        "\n",
        "Here are some of the main special tokens you'll see:\n",
        "\n",
        "- `xxbos`:: Indicates the beginning of a text (here, a review)\n",
        "- `xxmaj`:: Indicates the next word begins with a capital (since we lowercased everything)\n",
        "- `xxunk`:: Indicates the word is unknown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PUAKwtehOrt"
      },
      "source": [
        "### Numericalization with fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ibKNKNkhOru"
      },
      "source": [
        "*Numericalization* is the process of mapping tokens to integers. The steps are basically identical to those necessary to create a `Category` variable, such as the dependent variable of digits in MNIST:\n",
        "\n",
        "1. Make a list of all possible levels of that categorical variable (the vocab).\n",
        "1. Replace each level with its index in the vocab.\n",
        "\n",
        "Let's take a look at this in action on the word-tokenized text we saw earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "569VqVdahOru",
        "outputId": "5d47ac7f-0d4f-4a40-b994-c13e4d4818c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(#12) ['xxbos','xxmaj','winning','will','make','you','famous','.','xxmaj','losing','means','certain']\n"
          ]
        }
      ],
      "source": [
        "toks = tkn(txt)\n",
        "print(coll_repr(tkn(txt), 31))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ieg2wGmhOrv"
      },
      "source": [
        "Just like with `SubwordTokenizer`, we need to call `setup` on `Numericalize`; this is how we create the vocab. That means we'll need our tokenized corpus first. Since tokenization takes a while, it's done in parallel by fastai; but for this manual walkthrough, we'll use a small subset:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txts = df.loc[:2000,'book_desc']"
      ],
      "metadata": {
        "id": "dWmUlMv_2D7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymH5iw5qhOrv",
        "outputId": "e924f364-5ba6-4ab4-b49d-52d426ef7fed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#244) ['xxbos','xxmaj','winning','will','make','you','famous','.','xxmaj','losing'...]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "toks200 = txts[:200].map(tkn)\n",
        "toks200[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icm8BBa3hOrw"
      },
      "source": [
        "We can pass this to `setup` to create our vocab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8DXFbyDlhOrx",
        "outputId": "88d74777-3fb5-455b-e511-2d712ffc8643"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"(#1608) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj',',','the','and','of','.','a','to','in','is','-','his'...]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "num = Numericalize()\n",
        "num.setup(toks200)\n",
        "coll_repr(num.vocab,20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be1X7VwdhOry"
      },
      "source": [
        "Our special rules tokens appear first, and then every word appears once, in frequency order. The defaults to `Numericalize` are `min_freq=3,max_vocab=60000`. `max_vocab=60000` results in fastai replacing all words other than the most common 60,000 with a special *unknown word* token, `xxunk`. This is useful to avoid having an overly large embedding matrix, since that can slow down training and use up too much memory, and can also mean that there isn't enough data to train useful representations for rare words. However, this last issue is better handled by setting `min_freq`; the default `min_freq=3` means that any word appearing less than three times is replaced with `xxunk`.\n",
        "\n",
        "fastai can also numericalize your dataset using a vocab that you provide, by passing a list of words as the `vocab` parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nums = num(toks)[:20]; nums"
      ],
      "metadata": {
        "id": "y1nq_ByAAKZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6DlL12qhOr2"
      },
      "source": [
        "Now that we have numbers, we need to put them in batches for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aICVzVfehOr2"
      },
      "source": [
        "### Putting Our Texts into Batches for a Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPmpJLethOr3"
      },
      "source": [
        "When dealing with images, we needed to resize them all to the same height and width before grouping them together in a mini-batch so they could stack together efficiently in a single tensor. Here it's going to be a little different, because one cannot simply resize text to a desired length. Also, we want our language model to read text in order, so that it can efficiently predict what the next word is. This means that each new batch should begin precisely where the previous one left off.\n",
        "\n",
        "Suppose we have the following text:\n",
        "\n",
        "> : In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\n",
        "\n",
        "The tokenization process will add special tokens and deal with punctuation to return this text:\n",
        "\n",
        "> : xxbos xxmaj in this chapter , we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface . xxmaj first we will look at the processing steps necessary to convert text into numbers and how to customize it . xxmaj by doing this , we 'll have another example of the preprocessor used in the data block xxup api . \\n xxmaj then we will study how we build a language model and train it for a while .\n",
        "\n",
        "We now have 90 tokens, separated by spaces. Let's say we want a batch size of 6. We need to break this text into 6 contiguous parts of length 15:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "Vr3DapOBhOr4",
        "outputId": "b9999545-2530-413a-8a1c-c35c9cba8f63"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>back</td>\n",
              "      <td>over</td>\n",
              "      <td>the</td>\n",
              "      <td>example</td>\n",
              "      <td>of</td>\n",
              "      <td>classifying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>movie</td>\n",
              "      <td>reviews</td>\n",
              "      <td>we</td>\n",
              "      <td>studied</td>\n",
              "      <td>in</td>\n",
              "      <td>chapter</td>\n",
              "      <td>1</td>\n",
              "      <td>and</td>\n",
              "      <td>dig</td>\n",
              "      <td>deeper</td>\n",
              "      <td>under</td>\n",
              "      <td>the</td>\n",
              "      <td>surface</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>first</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>look</td>\n",
              "      <td>at</td>\n",
              "      <td>the</td>\n",
              "      <td>processing</td>\n",
              "      <td>steps</td>\n",
              "      <td>necessary</td>\n",
              "      <td>to</td>\n",
              "      <td>convert</td>\n",
              "      <td>text</td>\n",
              "      <td>into</td>\n",
              "      <td>numbers</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>how</td>\n",
              "      <td>to</td>\n",
              "      <td>customize</td>\n",
              "      <td>it</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>by</td>\n",
              "      <td>doing</td>\n",
              "      <td>this</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>'ll</td>\n",
              "      <td>have</td>\n",
              "      <td>another</td>\n",
              "      <td>example</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>preprocessor</td>\n",
              "      <td>used</td>\n",
              "      <td>in</td>\n",
              "      <td>the</td>\n",
              "      <td>data</td>\n",
              "      <td>block</td>\n",
              "      <td>xxup</td>\n",
              "      <td>api</td>\n",
              "      <td>.</td>\n",
              "      <td>\\n</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>then</td>\n",
              "      <td>we</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>will</td>\n",
              "      <td>study</td>\n",
              "      <td>how</td>\n",
              "      <td>we</td>\n",
              "      <td>build</td>\n",
              "      <td>a</td>\n",
              "      <td>language</td>\n",
              "      <td>model</td>\n",
              "      <td>and</td>\n",
              "      <td>train</td>\n",
              "      <td>it</td>\n",
              "      <td>for</td>\n",
              "      <td>a</td>\n",
              "      <td>while</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#hide_input\n",
        "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
        "tokens = tkn(stream)\n",
        "bs,seq_len = 6,15\n",
        "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFtTg-fchOr5"
      },
      "source": [
        "In a perfect world, we could then give this one batch to our model. But that approach doesn't scale, because outside of this toy example it's unlikely that a single batch containing all the texts would fit in our GPU memory (here we have 90 tokens, but all the IMDb reviews together give several million).\n",
        "\n",
        "So, we need to divide this array more finely into subarrays of a fixed sequence length. It is important to maintain order within and across these subarrays, because we will use a model that maintains a state so that it remembers what it read previously when predicting what comes next. \n",
        "\n",
        "Going back to our previous example with 6 batches of length 15, if we chose a sequence length of 5, that would mean we first feed the following array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "Za4Zy15ohOr5",
        "outputId": "8cbdb561-497d-4da4-8ad5-6f98911eb156"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>movie</td>\n",
              "      <td>reviews</td>\n",
              "      <td>we</td>\n",
              "      <td>studied</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>first</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>look</td>\n",
              "      <td>at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>how</td>\n",
              "      <td>to</td>\n",
              "      <td>customize</td>\n",
              "      <td>it</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>preprocessor</td>\n",
              "      <td>used</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>will</td>\n",
              "      <td>study</td>\n",
              "      <td>how</td>\n",
              "      <td>we</td>\n",
              "      <td>build</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#hide_input\n",
        "bs,seq_len = 6,5\n",
        "d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3iomSHnhOr6"
      },
      "source": [
        "Then this one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "PfkXK4aQhOr7",
        "outputId": "84041589-7e87-4626-ced4-e1b2f83cc03c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>chapter</td>\n",
              "      <td>1</td>\n",
              "      <td>and</td>\n",
              "      <td>dig</td>\n",
              "      <td>deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>the</td>\n",
              "      <td>processing</td>\n",
              "      <td>steps</td>\n",
              "      <td>necessary</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxmaj</td>\n",
              "      <td>by</td>\n",
              "      <td>doing</td>\n",
              "      <td>this</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>the</td>\n",
              "      <td>data</td>\n",
              "      <td>block</td>\n",
              "      <td>xxup</td>\n",
              "      <td>api</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>a</td>\n",
              "      <td>language</td>\n",
              "      <td>model</td>\n",
              "      <td>and</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#hide_input\n",
        "bs,seq_len = 6,5\n",
        "d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Lq_5LPhOr8"
      },
      "source": [
        "And finally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "X3ej54K3hOr8",
        "outputId": "47d786e4-15f4-4bb6-df8a-9ec534608f9d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>over</td>\n",
              "      <td>the</td>\n",
              "      <td>example</td>\n",
              "      <td>of</td>\n",
              "      <td>classifying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>under</td>\n",
              "      <td>the</td>\n",
              "      <td>surface</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>convert</td>\n",
              "      <td>text</td>\n",
              "      <td>into</td>\n",
              "      <td>numbers</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>we</td>\n",
              "      <td>'ll</td>\n",
              "      <td>have</td>\n",
              "      <td>another</td>\n",
              "      <td>example</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>.</td>\n",
              "      <td>\\n</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>then</td>\n",
              "      <td>we</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>it</td>\n",
              "      <td>for</td>\n",
              "      <td>a</td>\n",
              "      <td>while</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#hide_input\n",
        "bs,seq_len = 6,5\n",
        "d_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaOdRBFZhOr9"
      },
      "source": [
        "The first step in our dataset will be to transform the individual texts into a stream by concatenating them together. \n",
        "\n",
        "We then cut this stream into a certain number of batches (which is our *batch size*). For instance, if the stream has 50,000 tokens and we set a batch size of 10, this will give us 10 mini-streams of 5,000 tokens. What is important is that we preserve the order of the tokens (so from 1 to 5,000 for the first mini-stream, then from 5,001 to 10,000...), because we want the model to read continuous rows of text (as in the preceding example). An `xxbos` token is added at the start of each during preprocessing, so that the model knows when it reads the stream when a new entry is beginning.\n",
        "\n",
        "So to recap, at every epoch we shuffle our collection of documents and concatenate them into a stream of tokens. We then cut that stream into a batch of fixed-size consecutive mini-streams. Our model will then read the mini-streams in order, and thanks to an inner state, it will produce the same activation whatever sequence length we picked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFn7DDePhOsD"
      },
      "source": [
        "This concludes all the preprocessing steps we need to apply to our data. We are now ready to train our text classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ6TgoHBhOsD"
      },
      "source": [
        "## Training a Text Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8la9YnoFhOsE"
      },
      "source": [
        "As we saw at the beginning of this chapter, there are two steps to training a state-of-the-art text classifier using transfer learning: first we need to fine-tune our language model pretrained on Wikipedia to our corpus, and then we can use that model to train a classifier.\n",
        "\n",
        "As usual, let's start with assembling our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ep6OrZhhOsF"
      },
      "source": [
        "### Language Model Using DataBlock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emfy8-OVhOsG"
      },
      "source": [
        "fastai handles tokenization and numericalization automatically when `TextBlock` is passed to `DataBlock`. All of the arguments that can be passed to `Tokenize` and `Numericalize` can also be passed to `TextBlock`. In the next chapter we'll discuss the easiest ways to run each of these steps separately, to ease debugging—but you can always just debug by running them manually on a subset of your data as shown in the previous sections. And don't forget about `DataBlock`'s handy `summary` method, which is very useful for debugging data issues.\n",
        "\n",
        "Here's how we use `TextBlock` to create a language model, using fastai's defaults:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.text.all import *"
      ],
      "metadata": {
        "id": "1kHlYbLQSVyF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = df.sample(n=5000)"
      ],
      "metadata": {
        "id": "uhKdAngSss_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dls_lm = DataBlock(\n",
        "    blocks=TextBlock.from_df('book_desc', is_lm=True),get_x=ColReader('text'))\n",
        "\n",
        "dls_lm = dls_lm.dataloaders(df, bs=64)\n",
        "dls_lm.show_batch(max_n=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "a_CKie2fQ2uD",
        "outputId": "c31fae19-0968-42b3-ef46-40a097d73a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              ""
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj vladimir xxmaj nabokov : xxmaj sex , xxmaj lies , and xxmaj premium xxunk now , you undoubtedly know that xxmaj adrian xxmaj lyne 's film version of xxmaj vladimir xxmaj nabokov 's xxmaj lolita received its much xxunk xxmaj american debut on xxmaj august 2nd , on xxmaj xxunk , and will be released in theaters next month . xxmaj you very likely also know that the movie was , well , not exactly banned , but effectively so , by the refusal of xxmaj american film xxunk to participate in its release . xxmaj and you probably have a sense of what the xxunk was all about : xxmaj the film is reportedly too xxunk for xxmaj american eyes . xxmaj but there 's a very different xxunk on this story to be found through a closer look at the xxunk 's xxmaj lolita was originally</td>\n",
              "      <td>3.890000104904175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxbos xxup the xxup complete xxup robot is the definitive anthology of xxmaj asimov 's stunning visions of a robotic future … xxmaj in these stories , xxmaj isaac xxmaj asimov creates the xxmaj three xxmaj laws of xxmaj xxunk and xxunk in the xxmaj robot xxmaj age : when xxmaj earth is ruled by master - machines and when robots are more human than xxunk 9 • xxmaj introduction ( the xxmaj complete xxmaj robot ) • ( 1982 ) • essay by xxmaj isaac xxmaj asimov 15 • a xxmaj boy 's xxmaj best xxmaj friend • ( 1975 ) • short story by xxmaj isaac xxmaj asimov 19 • xxmaj sally • ( 1953 ) • short story by xxmaj isaac xxmaj asimov 41 • xxmaj someday • ( 1956 ) • short story by xxmaj isaac xxmaj asimov 55 • xxmaj point of xxmaj view •</td>\n",
              "      <td>4.349999904632568</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl8f7mOAhOsJ"
      },
      "source": [
        "Now that our data is ready, we can fine-tune the pretrained language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8OZ89OqhOsK"
      },
      "source": [
        "### Fine-Tuning the Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szS8F_URhOsK"
      },
      "source": [
        "To convert the integer word indices into activations that we can use for our neural network, we will use embeddings, just like we did for collaborative filtering and tabular modeling. Then we'll feed those embeddings into a *recurrent neural network* (RNN), using an architecture called *AWD-LSTM*. As we discussed earlier, the embeddings in the pretrained model are merged with random embeddings added for words that weren't in the pretraining vocabulary. This is handled automatically inside `language_model_learner`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "hVueQpbnhOsL"
      },
      "outputs": [],
      "source": [
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
        "    metrics=[accuracy, Perplexity()]).to_fp16()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypXTnv_whOsM"
      },
      "source": [
        "The loss function used by default is cross-entropy loss, since we essentially have a classification problem (the different categories being the words in our vocab). The *perplexity* metric used here is often used in NLP for language models: it is the exponential of the loss (i.e., `torch.exp(cross_entropy)`). We  also include the accuracy metric, to see how many times our model is right when trying to predict the next word, since cross-entropy (as we've seen) is both hard to interpret, and tells us more about the model's confidence than its accuracy.\n",
        "\n",
        "Let's go back to the process diagram from the beginning of this chapter. The first arrow has been completed for us and made available as a pretrained model in fastai, and we've just built the `DataLoaders` and `Learner` for the second stage. Now we're ready to fine-tune our language model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOqdgNzPhOsN"
      },
      "source": [
        "It takes quite a while to train each epoch, so we'll be saving the intermediate model results during the training process. Since `fine_tune` doesn't do that for us, we'll use `fit_one_cycle`. Just like `vision_learner`, `language_model_learner` automatically calls `freeze` when using a pretrained model (which is the default), so this will only train the embeddings (the only part of the model that contains randomly initialized weights—i.e., embeddings for words that are in our IMDb vocab, but aren't in the pretrained model vocab):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "ioICh4OIhOsN",
        "outputId": "2ca93b4c-9e34-4ac7-84e1-63b66b42be8e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.112721</td>\n",
              "      <td>3.988225</td>\n",
              "      <td>0.308627</td>\n",
              "      <td>53.959015</td>\n",
              "      <td>16:20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#torch.cuda.empty_cache()\n",
        "learn.fit_one_cycle(1, 2e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nqnYHaKhOsO"
      },
      "source": [
        "### Saving and Loading Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l38IGsdshOsO"
      },
      "source": [
        "You can easily save the state of your model like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF5k3QTohOsP",
        "outputId": "8763ba34-4a3f-45dc-a4e6-38a32d364c6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/content/gdrive/MyDrive/NLP/1epoch.pth')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "learn.save('/content/gdrive/MyDrive/NLP/1epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwW4mY4EhOsP"
      },
      "source": [
        "This will create a file in `learn.path/models/` named *1epoch.pth*. If you want to load your model in another machine after creating your `Learner` the same way, or resume training later, you can load the content of this file with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Cb8jKftJhOsP"
      },
      "outputs": [],
      "source": [
        "learn = learn.load('/content/gdrive/MyDrive/NLP/1epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twsOguwHhOsQ"
      },
      "source": [
        "Once the initial training has completed, we can continue fine-tuning the model after unfreezing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "klPIvk1vhOsQ",
        "outputId": "b3536edc-36c5-49f0-d609-3f63411a8d00"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='5' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      50.00% [5/10 1:25:11<1:25:11]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.737280</td>\n",
              "      <td>3.859213</td>\n",
              "      <td>0.325504</td>\n",
              "      <td>47.428028</td>\n",
              "      <td>16:56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.685014</td>\n",
              "      <td>3.810924</td>\n",
              "      <td>0.330890</td>\n",
              "      <td>45.192158</td>\n",
              "      <td>17:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.570149</td>\n",
              "      <td>3.761614</td>\n",
              "      <td>0.337602</td>\n",
              "      <td>43.017784</td>\n",
              "      <td>17:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.411179</td>\n",
              "      <td>3.748586</td>\n",
              "      <td>0.341668</td>\n",
              "      <td>42.460987</td>\n",
              "      <td>16:57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.280570</td>\n",
              "      <td>3.750224</td>\n",
              "      <td>0.345136</td>\n",
              "      <td>42.530613</td>\n",
              "      <td>17:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "      <progress value='216' class='' max='1509' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      14.31% [216/1509 02:07<12:41 2.9995]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(10, 2e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cae0tiQihOsR"
      },
      "source": [
        "Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the *encoder*. We can save it with `save_encoder`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLLz1LQRhOsR"
      },
      "outputs": [],
      "source": [
        "learn.save_encoder('/content/gdrive/MyDrive/NLP/finetuned')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu5_WU81hOsR"
      },
      "source": [
        "> jargon: Encoder: The model not including the task-specific final layer(s). This term means much the same thing as _body_ when applied to vision CNNs, but \"encoder\" tends to be more used for NLP and generative models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P_l3-AkhOsR"
      },
      "source": [
        "This completes the second stage of the text classification process: fine-tuning the language model. We can now use it to fine-tune a classifier using the IMDb sentiment labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kWzv6yThOsU"
      },
      "source": [
        "### Creating the Classifier DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9akSfvO-hOsU"
      },
      "source": [
        "We're now moving from language model fine-tuning to classifier fine-tuning. To recap, a language model predicts the next word of a document, so it doesn't need any external labels. A classifier, however, predicts some external label.\n",
        "\n",
        "This means that the structure of our `DataBlock` for NLP classification will look very familiar. It's actually nearly the same as we've seen for the many image classification datasets we've worked with.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's crete from the rating the categorical variable for positive rating above 3.5 over 5 and negative below that."
      ],
      "metadata": {
        "id": "NAD5FrNGC-2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['book_rating'] = np.where(df['book_rating'] > 3.5, 'Pos', 'Neg')"
      ],
      "metadata": {
        "id": "Hd9KBpyfiEdx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create the data block for the classification task"
      ],
      "metadata": {
        "id": "2njxUfTYDHxG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "9iL824u6hOsU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "0f44f29f-6924-4e16-c732-54220d948f00"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              ""
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj spiritus xxmaj mundi by xxmaj robert xxmaj sheppard , nominated for the prestigious 2014 xxmaj pushcart xxmaj prize for xxmaj literature , consists of xxmaj spiritus xxmaj mundi , the novel — book xxup i , and xxmaj spiritus xxmaj mundi , the romance — book xxup ii . xxmaj book xxmaj i ’s espionage - terror - political - religious thriller - action criss - crosses the globe from xxmaj beijing to xxmaj london to xxmaj washington , xxmaj mexico xxmaj city and xxmaj jerusalem presenting a vast panorama of the contemporary international world , including compelling action , deep and realistic characters and surreal adventures , while xxmaj book xxup ii xxunk the setting and scope into a fantasy ( though still rooted in the real ) adventure where the protagonists embark on a quest to the realms of xxmaj middle xxmaj earth and its xxmaj</td>\n",
              "      <td>Pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxbos xxmaj another cutting - edge thriller set at the intersection of science , religion , and history from the bestselling author of xxmaj the xxmaj last xxunk xxmaj york xxmaj times bestselling author xxmaj raymond xxmaj khoury , whose debut novel , xxmaj the xxmaj last xxmaj templar , has sold more than a million copies in the xxmaj united xxmaj states , and whose second , xxmaj the xxmaj sanctuary , was also a major national bestseller , returns with xxmaj the xxmaj sign . xxmaj like the first two , this new thriller combines gripping contemporary suspense with a high - concept mystery rooted in history , philosophy , religion , and science . xxmaj and like those novels , it is bound for bestseller lists nationwide . \\r\\r\\r\\n xxmaj in xxmaj antarctica , a scientific expedition drops anchor for a live news feed . xxmaj as</td>\n",
              "      <td>Neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "imdb_clas = DataBlock(\n",
        "    blocks=(TextBlock.from_df('book_desc', seq_len=72,vocab=dls_lm.vocab), CategoryBlock),\n",
        "    get_x=ColReader('text'), get_y=ColReader('book_rating'))\n",
        "\n",
        "dls_clas = imdb_clas.dataloaders(df, bs=64)\n",
        "dls_clas.show_batch(max_n=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We validate that the vocabulary of the language model and the downstream task model is the same:"
      ],
      "metadata": {
        "id": "_Ml0m4kHDQBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(dls_clas.vocab[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPeScYFFnh2o",
        "outputId": "0f724797-a01c-4f5b-d353-017ba32a2e1b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51296"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dls.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDhmeCk2n_CY",
        "outputId": "f8171ac1-8826-4c09-ab31-1130c49ddf52"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZlJ_r4chOsY"
      },
      "source": [
        "Looking at the `DataBlock` definition, every piece is familiar from previous data blocks we've built, with two important exceptions:\n",
        "\n",
        "- `TextBlock.from_df` no longer has the `is_lm=True` parameter.\n",
        "- We pass the `vocab` we created for the language model fine-tuning.\n",
        "\n",
        "The reason that we pass the `vocab` of the language model is to make sure we use the same correspondence of token to index. Otherwise the embeddings we learned in our fine-tuned language model won't make any sense to this model, and the fine-tuning step won't be of any use.\n",
        "\n",
        "By passing `is_lm=False` (or not passing `is_lm` at all, since it defaults to `False`) we tell `TextBlock` that we have regular labeled data, rather than using the next tokens as labels. There is one challenge we have to deal with, however, which is to do with collating multiple documents into a mini-batch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AIKXyRJhOsZ"
      },
      "source": [
        "Remember, PyTorch `DataLoader`s need to collate all the items in a batch into a single tensor, and a single tensor has a fixed shape (i.e., it has some particular length on every axis, and all items must be consistent). This should sound familiar: we had the same issue with images. In that case, we use padding as cropping and other augmentation do not make sense!\n",
        "\n",
        "We will expand the shortest texts to make them all the same size. To do this, we use a special padding token that will be ignored by our model. Additionally, to avoid memory issues and improve performance, we will batch together texts that are roughly the same lengths (with some shuffling for the training set). We do this by (approximately, for the training set) sorting the documents by length prior to each epoch. The result of this is that the documents collated into a single batch will tend to be of similar lengths. We won't pad every batch to the same size, but will instead use the size of the largest document in each batch as the target size. \n",
        "\n",
        "The sorting and padding are automatically done by the data block API for us when using a `TextBlock`, with `is_lm=False`. (We don't have this same issue for language model data, since we concatenate all the documents together first, and then split them into equally sized sections.)\n",
        "\n",
        "We can now create a model to classify our texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "vGE0DNdYhOsa"
      },
      "outputs": [],
      "source": [
        "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
        "                                metrics=accuracy).to_fp16()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-V0lEZphOsb"
      },
      "source": [
        "The final step prior to training the classifier is to load the encoder from our fine-tuned language model. We use `load_encoder` instead of `load` because we only have pretrained weights available for the encoder; `load` by default raises an exception if an incomplete model is loaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Fn2EfZUXhOsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "905353fd-8a83-4891-a3d8-ee240451ff32"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-b211eaa9fb34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/NLP/finetuned'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/text/learner.py\u001b[0m in \u001b[0;36mload_encoder\u001b[0;34m(self, file, device)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mdistrib_barrier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mwgts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_path_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_raw_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwgts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1605\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AWD_LSTM:\n\tsize mismatch for encoder.weight: copying a param with shape torch.Size([51304, 400]) from checkpoint, the shape in current model is torch.Size([51296, 400]).\n\tsize mismatch for encoder_dp.emb.weight: copying a param with shape torch.Size([51304, 400]) from checkpoint, the shape in current model is torch.Size([51296, 400])."
          ]
        }
      ],
      "source": [
        "learn = learn.load_encoder('/content/gdrive/MyDrive/NLP/finetuned')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9mHZhqPhOsc"
      },
      "source": [
        "### Fine-Tuning the Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvVw3xflhOsc"
      },
      "source": [
        "The last step is to train with discriminative learning rates and *gradual unfreezing*. In computer vision we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "L7_BxMIfhOsc",
        "outputId": "2b8b5782-e71b-4e2a-dfcd-247639f537af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.246642</td>\n",
              "      <td>0.180298</td>\n",
              "      <td>0.929480</td>\n",
              "      <td>01:50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCp3ksqmhOsd"
      },
      "source": [
        "In just one epoch we get the same result as our training in <<chapter_intro>>: not too bad! We can pass `-2` to `freeze_to` to freeze all except the last two parameter groups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "6FEn3wkBhOsd",
        "outputId": "4464f6de-a349-4b16-c1a8-707dcd4b5d86"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.227928</td>\n",
              "      <td>0.163428</td>\n",
              "      <td>0.937920</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42lGvO4hhOse"
      },
      "source": [
        "Then we can unfreeze a bit more, and continue training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "-DQ07CSmhOse",
        "outputId": "807d0097-100b-4452-dff7-1d2603058409"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.189323</td>\n",
              "      <td>0.152421</td>\n",
              "      <td>0.942360</td>\n",
              "      <td>02:52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrfHhXKnhOsf"
      },
      "source": [
        "And finally, the whole model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "VKwqcf9hhOsf",
        "outputId": "1b3f7876-6f59-4b2b-9f08-efa3c003b718"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.159610</td>\n",
              "      <td>0.149004</td>\n",
              "      <td>0.944680</td>\n",
              "      <td>03:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.147273</td>\n",
              "      <td>0.148915</td>\n",
              "      <td>0.944720</td>\n",
              "      <td>03:29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_btQrIrhOsg"
      },
      "source": [
        "We reached 94.3% accuracy, which was state-of-the-art performance just three years ago. By training another model on all the texts read backwards and averaging the predictions of those two models, we can even get to 95.1% accuracy, which was the state of the art introduced by the ULMFiT paper. It was only beaten a few months ago, by fine-tuning a much bigger model and using expensive data augmentation techniques (translating sentences in another language and back, using another model for translation).\n",
        "\n",
        "Using a pretrained model let us build a fine-tuned language model that was pretty powerful, to either generate fake reviews or help classify them. This is exciting stuff, but it's good to remember that this technology can also be used for malign purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtDx3-ouhOsi"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPmc8EFuhOsj"
      },
      "source": [
        "In this chapter we explored the last application covered out of the box by the fastai library: text. We saw two types of models: language models that can generate texts, and a classifier that determines if a review is positive or negative. To build a state-of-the art classifier, we used a pretrained language model, fine-tuned it to the corpus of our task, then used its body (the encoder) with a new head to do the classification.\n",
        "\n",
        "Before we end this section, we'll take a look at how the fastai library can help you assemble your data for your specific problems."
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "10_nlp.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}