{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAZckrRIsYzssAZUqQICjH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afortuny/DeepLearningFastAI/blob/main/15_Architectures_Deep_Dive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application Architectures Deep Dive"
      ],
      "metadata": {
        "id": "oCF3z8WhpE8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter, we're going to fill in all the missing details on how fastai's application models work and show you how to build the models they use."
      ],
      "metadata": {
        "id": "97Z5RbS2qoDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZcG5roqqA-g",
        "outputId": "b1161fb2-2884-4130-d8f1-45cf078cf8e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 719 kB 28.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 431 kB 74.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 71.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 72.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 163 kB 70.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 73.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 125.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 86.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 70.2 MB/s \n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "from fastbook import *"
      ],
      "metadata": {
        "id": "5RkLqQOOqDCl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision"
      ],
      "metadata": {
        "id": "a6qLAEr0qKwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For computer vision application we use the functions vision_learner and unet_learner to build our models, depending on the task. In this section we'll explore how to build the Learner objects we used in Parts 1 and 2 of this book."
      ],
      "metadata": {
        "id": "Nb91GF-0qreW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### vision_learner"
      ],
      "metadata": {
        "id": "pdRCeLLfqxk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at what happens when we use the vision_learner function. We begin by passing this function an architecture to use for the body of the network. Most of the time we use a ResNet, pretrained weights are downloaded as required and loaded into the ResNet."
      ],
      "metadata": {
        "id": "78AOBkIRqzMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, for transfer learning, the network needs to be cut. This refers to slicing off the final layer, which is only responsible for ImageNet-specific categorization. In fact, we do not slice off only this layer, but everything from the adaptive average pooling layer onwards. The reason for this will become clear in just a moment. Since different architectures might use different types of pooling layers, or even completely different kinds of heads, we don't just search for the adaptive pooling layer to decide where to cut the pretrained model. Instead, we have a dictionary of information that is used for each model to determine where its body ends, and its head starts. We call this model_meta—here it is for resnet-50:"
      ],
      "metadata": {
        "id": "vSHmsihJrJ4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_meta[resnet50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACczN8tcrMI2",
        "outputId": "1a45590c-4667-4e8e-e5d5-69d464c95560"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cut': -2,\n",
              " 'split': <function fastai.vision.learner._resnet_split(m)>,\n",
              " 'stats': ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we take all of the layers prior to the cut point of -2, we get the part of the model that fastai will keep for transfer learning. Now, we put on our new head. This is created using the function create_head:"
      ],
      "metadata": {
        "id": "Vd4ta0LPrTip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "create_head(20,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vSlB7UbrYSO",
        "outputId": "eebacb4c-3f54-423c-d579-770479a58f71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): AdaptiveConcatPool2d(\n",
              "    (ap): AdaptiveAvgPool2d(output_size=1)\n",
              "    (mp): AdaptiveMaxPool2d(output_size=1)\n",
              "  )\n",
              "  (1): fastai.layers.Flatten(full=False)\n",
              "  (2): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (3): Dropout(p=0.25, inplace=False)\n",
              "  (4): Linear(in_features=40, out_features=512, bias=False)\n",
              "  (5): ReLU(inplace=True)\n",
              "  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (7): Dropout(p=0.5, inplace=False)\n",
              "  (8): Linear(in_features=512, out_features=2, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this function you can choose how many additional linear layers are added to the end, how much dropout to use after each one, and what kind of pooling to use. By default, fastai will apply both average pooling, and max pooling, and will concatenate the two together (this is the AdaptiveConcatPool2d layer). This is not a particularly common approach, but it was developed independently at fastai and other research labs in recent years, and tends to provide some small improvement over using just average pooling."
      ],
      "metadata": {
        "id": "MWDq_PKlriYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***fastai*** is a bit different from most libraries in that by default it adds two linear layers, rather than one, in the CNN head. The reason for this is that transfer learning can still be useful even, as we have seen, when transferring the pretrained model to very different domains. However, just using a single linear layer is unlikely to be enough in these cases; we have found that using two linear layers can allow transfer learning to be used more quickly and easily, in more situations."
      ],
      "metadata": {
        "id": "AuR6cRrkrtYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now take a look at what unet_learner do for segmentation problems"
      ],
      "metadata": {
        "id": "6cz1Rwm6r8Cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unet_learner"
      ],
      "metadata": {
        "id": "iV-Rwntyr-fI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most interesting architectures in deep learning is the one that we used for segmentation. Segmentation is a challenging task, because the output required is really an image, or a pixel grid, containing the predicted label for every pixel. There are other tasks that share a similar basic design, such as increasing the resolution of an image (super-resolution), adding color to a black-and-white image (colorization), or converting a photo into a synthetic painting (style transfer). In each case, we are starting with an image and converting it to some other image of the same dimensions or aspect ratio, but with the pixels altered in some way. We refer to these as generative vision models."
      ],
      "metadata": {
        "id": "xrY-1-ErsHSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naturally, we do this with a neural network! So we need some kind of layer that can increase the grid size in a CNN. One very simple approach to this is to replace every pixel in the 7×7 grid with four pixels in a 2×2 square. Each of those four pixels will have the same value—this is known as nearest neighbor interpolation. PyTorch provides a layer that does this for us, so one option is to create a head that contains stride-1 convolutional layers (along with batchnorm and ReLU layers as usual) interspersed with 2×2 nearest neighbor interpolation layers.Another approach is to replace the nearest neighbor and convolution combination with a transposed convolution, otherwise known as a stride half convolution. This is identical to a regular convolution, but first zero padding is inserted between all the pixels in the input. See the [arithmetic convolution paper](https://arxiv.org/abs/1603.07285) for a visual inspection.The result of this is to increase the size of the input. You can try this out now by using fastai's ConvLayer class; pass the parameter transpose=True to create a transposed convolution, instead of a regular one, in your custom head."
      ],
      "metadata": {
        "id": "6KoJXfYxsm7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neither of these approaches, however, works really well. The problem is that our 7×7 grid simply doesn't have enough information to create a 224×224-pixel output. It's asking an awful lot of the activations of each of those grid cells to have enough information to fully regenerate every pixel in the output. The solution to this problem is to use skip connections, like in a ResNet, but skipping from the activations in the body of the ResNet all the way over to the activations of the transposed convolution on the opposite side of the architecture. This approach, was developed by Olaf Ronneberger, Philipp Fischer, and Thomas Brox in the 2015 paper \"U-Net: Convolutional Networks for Biomedical Image Segmentation\". Although the paper focused on medical applications, the U-Net has revolutionized all kinds of generative vision models."
      ],
      "metadata": {
        "id": "4xBfJQepty9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this architecture, the input to the transposed convolutions is not just the lower-resolution grid in the preceding layer, but also the higher-resolution grid in the ResNet head. This allows the U-Net to use all of the information of the original image, as it is needed. One challenge with U-Nets is that the exact architecture depends on the image size. fastai has a unique DynamicUnet class that autogenerates an architecture of the right size based on the data provided."
      ],
      "metadata": {
        "id": "_5rzJ9l1wk-Z"
      }
    }
  ]
}